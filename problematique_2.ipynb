{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "",
  "signature": "sha256:3fa4a96c3221e8447bcc3993a99fadfc81e33815414cbc9badf8b5a2068496de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pr\u00e9dire la victoire d'une \u00e9quipe"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ismael Bonneau et Issam Benamara\n",
      "\n",
      "donn\u00e9es: https://www.kaggle.com/hugomathien/soccer\n",
      "\n",
      "inspiration et remerciements : http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ismael Bonneau et Issam Benamara"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Description du probl\u00e8me"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pr\u00e9dire les victoires\n",
      "\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mod\u00e8le"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "comparaison de diff\u00e9rents mod\u00e8les\n",
      "\n",
      "blaaablablabla"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sqlite3\n",
      "from sklearn.model_selection import train_test_split\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "tables : Player_Attributes, Player, Team, Team_Attributes, Match, League, Country\n",
      "\"\"\"\n",
      "from datetime import datetime\n",
      "\n",
      "path = \"./data/\"  #chemin vers la base de donn\u00e9es\n",
      "\n",
      "database = path + 'soccer.sqlite'\n",
      "\n",
      "\n",
      "conn = sqlite3.connect(database)\n",
      "\n",
      "attributes = [\"home_team_goal\", \"away_team_goal\"]\n",
      "homeAtt = [\"buildUpPlayDribbling\", \"buildUpPlayPassing\", \"defencePressure\", \"defenceTeamWidth\", \n",
      "              \"defenceAggression\", \"buildUpPlaySpeed\", \"chanceCreationCrossing\", \"chanceCreationShooting\",\n",
      "              \"chanceCreationPassing\", \"buildUpPlayPassing\"]\n",
      "homeAtt = [\"t1.\" + s + \" AS \"+ s + \"Home\" for s in homeAtt]\n",
      "awayAtt = [\"buildUpPlayDribbling\", \"buildUpPlayPassing\", \"defencePressure\", \"defenceTeamWidth\", \n",
      "              \"defenceAggression\", \"buildUpPlaySpeed\", \"chanceCreationCrossing\", \"chanceCreationShooting\",\n",
      "              \"chanceCreationPassing\", \"buildUpPlayPassing\"]\n",
      "awayAtt = [\"t2.\" + s + \" AS \"+ s + \"Away\" for s in awayAtt]\n",
      "\n",
      "att = homeAtt + awayAtt + attributes\n",
      "\n",
      "query = \"SELECT \"\n",
      "query += \", \".join(att)\n",
      "query += \" FROM Team_Attributes t1, Team_Attributes t2, Match\"\n",
      "query += \" WHERE t1.team_api_id=home_team_api_id AND t2.team_api_id=away_team_api_id\"\n",
      "\n",
      "matches = pd.read_sql(query, conn)\n",
      "matches = matches.fillna(value=50)\n",
      "conn.close()\n",
      "\n",
      "print(\"notre table contient \" + str(matches.shape[0]) + \" enregistrements d'equipes\")\n",
      "print(matches.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "notre table contient 781625 enregistrements d'equipes\n",
        "(781625, 22)\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "victoires = matches[\"home_team_goal\"] > matches[\"away_team_goal\"]\n",
      "\n",
      "Y = []\n",
      "for i in range(len(victoires)):\n",
      "    if victoires[i]:\n",
      "        Y.append(1)\n",
      "    else:\n",
      "        Y.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = np.array([Y])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(Y.shape)\n",
      "Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 781625)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "array([[0, 0, 0, ..., 1, 0, 1]])"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matches = matches.drop([\"away_team_goal\", \"home_team_goal\"], axis=1)\n",
      "X = matches.as_matrix().T\n",
      "print(X.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(20, 781625)\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = train_test_split(X.T, Y.T, test_size=0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = X_train.T\n",
      "X_test = X_test.T\n",
      "y_train = y_train.T\n",
      "y_test = y_test.T\n",
      "\n",
      "print(\"X_train shape: \"+str(X_train.shape))\n",
      "print(\"X_test shape: \"+str(X_test.shape))\n",
      "print(\"y_train shape: \"+str(y_train.shape))\n",
      "print(\"y_test shape: \"+str(y_test.shape))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X_train shape: (20, 742543)\n",
        "X_test shape: (20, 39082)\n",
        "y_train shape: (1, 742543)\n",
        "y_test shape: (1, 39082)\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class simplePerceptron:\n",
      "    \"\"\"\n",
      "    classe impl\u00e9mentant le perceptron classique,\n",
      "    avec simoid comme fonction d'activation et descente de gradient.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, inputdim):\n",
      "        self.inputdim = inputdim\n",
      "    \n",
      "    \n",
      "    def sigmoid(self, x):\n",
      "        \"\"\"\n",
      "        Calcule sigmoid(x)\n",
      "        \"\"\"\n",
      "        return 1.0 / (1+np.exp(-x))\n",
      "    \n",
      "    def initialiser(self, dim):\n",
      "        \"\"\"\n",
      "        initialise le vecteur w al\u00e9atoirement et le biais b\n",
      "        \"\"\"\n",
      "        self.w = np.random.randn(dim, 1) * 0.01\n",
      "        self.b = 0\n",
      "\n",
      "    def propagate(self, X, Y):\n",
      "        \"\"\"\n",
      "        forward propagation de la matrice X\n",
      "        X contient m exemples qui sont propag\u00e9s en une seule fois.\n",
      "        retourne le cost (moyenne des m erreurs)\n",
      "\n",
      "        utilise la fonction \"Cross-entropy loss\"\n",
      "        \"\"\"\n",
      "    \n",
      "        m = X.shape[1]\n",
      "        A = sigmoid(np.dot(self.w.T, X) + self.b)\n",
      "\n",
      "        cost = -1.0/m*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
      "\n",
      "        dw = 1/m*np.dot(X, (A - Y).T)\n",
      "        db = 1/m*np.sum((A - Y))\n",
      "\n",
      "        cost = np.squeeze(cost)\n",
      "\n",
      "        gradients = {\"dw\": dw,\n",
      "                 \"db\": db}\n",
      "\n",
      "        return gradients, cost\n",
      "    \n",
      "\n",
      "    def optimise(self, X, Y, nbiterations=2000, learning_rate=0.01, verbose=False):\n",
      "        \"\"\"\n",
      "        optimisation de w et b pour m exemples en m\u00eame temps.\n",
      "        \"\"\"\n",
      "\n",
      "        costs = []\n",
      "\n",
      "        for i in range(nbiterations):\n",
      "            #forward propagation\n",
      "            gradients, cost = self.propagate(X, Y)\n",
      "\n",
      "            #on r\u00e9cup\u00e8re les gradients\n",
      "            dw = gradients[\"dw\"]\n",
      "            db = gradients[\"db\"]\n",
      "            #on met \u00e0 jour w et b\n",
      "            self.w = self.w - learning_rate * dw\n",
      "            self.b = self.b - learning_rate * db\n",
      "\n",
      "            if i % 100 == 0:\n",
      "                costs.append(cost)\n",
      "\n",
      "        params = {\"w\": self.w,\n",
      "                  \"b\": self.b}\n",
      "\n",
      "        return params, costs\n",
      "\n",
      "    \n",
      "    \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        prediction sur un ensemble de m exemples\n",
      "        renvoie 0 ou 1\n",
      "        \"\"\"\n",
      "\n",
      "        m = X.shape[1]\n",
      "        Y_prediction = np.zeros((1,m))\n",
      "        w = w.reshape(X.shape[0], 1)\n",
      "\n",
      "        A = self.sigmoid(np.dot(self.w.T, X) + self.b)\n",
      "\n",
      "        for i in range(A.shape[1]):\n",
      "            Y_prediction = np.where(A>0.5, 1, 0)\n",
      "\n",
      "        return Y_prediction\n",
      "\n",
      "\n",
      "    def run(self, X_train, Y_train, X_test, Y_test, nbiterations=2000, learning_rate=0.01):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "    \n",
      "        self.initialiser(X_train.shape[0])\n",
      "    \n",
      "        parameters, costs = self.optimise(X_train, Y_train, nbiterations, learning_rate, verbose=False)\n",
      "    \n",
      "        self.w = parameters[\"w\"]\n",
      "        self.b = parameters[\"b\"]\n",
      "    \n",
      "        # calculer l'accuracy test / train\n",
      "        Y_prediction_test = self.predict(w, b, X_test)\n",
      "        Y_prediction_train = self.predict(w, b, X_train)\n",
      "    \n",
      "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
      "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
      "    \n",
      "        return parameters, costs, Y_prediction_test, Y_prediction_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "perceptron = simplePerceptron(X.shape[0])\n",
      "\n",
      "iterations = 2000\n",
      "_ , costs, _ , _ = perceptron.run(X_train, y_train, X_test, y_test, iterations, learning_rate=0.01)\n",
      "\n",
      "plt.title(\"evolution du cost au cours du training\")\n",
      "plt.xlabel(\"#iterations\")\n",
      "plt.ylabel(\"cost\")\n",
      "plt.plot(range(iterations), costs)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class hiddenLayerPerceptron:\n",
      "    \"\"\"\n",
      "    impl\u00e9mente un r\u00e9seau de neurones \u00e0 un seul hidden layer.\n",
      "    mod\u00e8le : tanh -> sigmoid\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, inputdimension, hiddendim):\n",
      "        self.inputdimension = inputdimension\n",
      "        self.hiddendim = hiddendim\n",
      "        \n",
      "    def cost(self, A, Y):\n",
      "        \"\"\"\n",
      "        retourne la moyenne de la cross-entropy loss\n",
      "        sur les m exemples contenus dans A\n",
      "        \"\"\"\n",
      "        return -1.0/m*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
      "    \n",
      "    def initialize_parameters(self, X, X):\n",
      "        \"\"\"\n",
      "\n",
      "        \"\"\"\n",
      "        # w et b du input layer :\n",
      "        w1 = np.random.randn(self.hiddendim, X.shape[0]) * 0.01\n",
      "        b1 = np.zeros((self.hiddendim 1))\n",
      "\n",
      "        # w et b hiden layer :\n",
      "        w2 = np.random.randn(X.shape[1], self.hiddendim) * 0.01\n",
      "        b2 = np.zeros((X.shape[1], 1))\n",
      "\n",
      "        parameters = {\"w1\": w1,\n",
      "                      \"b1\": b1,\n",
      "                      \"w2\": w2,\n",
      "                      \"b2\": b2}\n",
      "\n",
      "        return parameters\n",
      "\n",
      "    def "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}