{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "",
  "signature": "sha256:19322d870d1bac2ad7cea2e50c134aced2c2f87dcf069d85d6ebac70503c23ce"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pr\u00e9dire la victoire d'une \u00e9quipe"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ismael Bonneau et Issam Benamara\n",
      "\n",
      "donn\u00e9es: https://www.kaggle.com/hugomathien/soccer\n",
      "\n",
      "inspiration et remerciements : http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Description du probl\u00e8me"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pr\u00e9dire les victoires\n",
      "\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla\n",
      "blablabla"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mod\u00e8le"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "comparaison de diff\u00e9rents mod\u00e8les\n",
      "\n",
      "perceptron simple vs r\u00e9seau de neurones \u00e0 un hidden layer\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sqlite3\n",
      "from sklearn.model_selection import train_test_split\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/pandas-0.20.2-py2.7-linux-x86_64.egg/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
        "The minimum supported version is 2.4.6\n",
        "\n",
        "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "tables : Player_Attributes, Player, Team, Team_Attributes, Match, League, Country\n",
      "\"\"\"\n",
      "from datetime import datetime\n",
      "\n",
      "path = \"./data/\"  #chemin vers la base de donn\u00e9es\n",
      "\n",
      "database = path + 'soccer.sqlite'\n",
      "\n",
      "\n",
      "conn = sqlite3.connect(database)\n",
      "\n",
      "attributes = [\"home_team_goal\", \"away_team_goal\"]\n",
      "homeAtt = [\"buildUpPlayDribbling\", \"buildUpPlayPassing\", \"defencePressure\", \"defenceTeamWidth\", \n",
      "              \"defenceAggression\", \"buildUpPlaySpeed\", \"chanceCreationCrossing\", \"chanceCreationShooting\",\n",
      "              \"chanceCreationPassing\", \"buildUpPlayPassing\"]\n",
      "homeAtt = [\"t1.\" + s + \" AS \"+ s + \"Home\" for s in homeAtt]\n",
      "awayAtt = [\"buildUpPlayDribbling\", \"buildUpPlayPassing\", \"defencePressure\", \"defenceTeamWidth\", \n",
      "              \"defenceAggression\", \"buildUpPlaySpeed\", \"chanceCreationCrossing\", \"chanceCreationShooting\",\n",
      "              \"chanceCreationPassing\", \"buildUpPlayPassing\"]\n",
      "awayAtt = [\"t2.\" + s + \" AS \"+ s + \"Away\" for s in awayAtt]\n",
      "\n",
      "att = homeAtt + awayAtt + attributes\n",
      "\n",
      "query = \"SELECT \"\n",
      "query += \", \".join(att)\n",
      "query += \" FROM Team_Attributes t1, Team_Attributes t2, Match\"\n",
      "query += \" WHERE t1.team_api_id=home_team_api_id AND t2.team_api_id=away_team_api_id\"\n",
      "\n",
      "matches = pd.read_sql(query, conn)\n",
      "matches = matches.fillna(value=50)\n",
      "conn.close()\n",
      "\n",
      "print(\"notre table contient \" + str(matches.shape[0]) + \" enregistrements d'equipes\")\n",
      "print(matches.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "notre table contient 781625 enregistrements d'equipes\n",
        "(781625, 22)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "victoires = matches[\"home_team_goal\"] > matches[\"away_team_goal\"]\n",
      "\n",
      "Y = []\n",
      "for i in range(len(victoires)):\n",
      "    if victoires[i]:\n",
      "        Y.append(1)\n",
      "    else:\n",
      "        Y.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = np.array([Y])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(Y.shape)\n",
      "Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 781625)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([[0, 0, 0, ..., 1, 0, 1]])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matches = matches.drop([\"away_team_goal\", \"home_team_goal\"], axis=1)\n",
      "X = matches.as_matrix().T\n",
      "print(X.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(20, 781625)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = train_test_split(X.T, Y.T, test_size=0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = X_train.T\n",
      "X_test = X_test.T\n",
      "y_train = y_train.T\n",
      "y_test = y_test.T\n",
      "\n",
      "print(\"X_train shape: \"+str(X_train.shape))\n",
      "print(\"X_test shape: \"+str(X_test.shape))\n",
      "print(\"y_train shape: \"+str(y_train.shape))\n",
      "print(\"y_test shape: \"+str(y_test.shape))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X_train shape: (20, 742543)\n",
        "X_test shape: (20, 39082)\n",
        "y_train shape: (1, 742543)\n",
        "y_test shape: (1, 39082)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class simplePerceptron:\n",
      "    \"\"\"\n",
      "    classe impl\u00e9mentant le perceptron classique,\n",
      "    avec simoid comme fonction d'activation et descente de gradient.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, inputdim):\n",
      "        self.inputdim = inputdim\n",
      "    \n",
      "    \n",
      "    def sigmoid(self, x):\n",
      "        \"\"\"\n",
      "        Calcule sigmoid(x)\n",
      "        \"\"\"\n",
      "        return 1.0 / (1+np.exp(-x))\n",
      "    \n",
      "    def initialiser(self, dim):\n",
      "        \"\"\"\n",
      "        initialise le vecteur w al\u00e9atoirement et le biais b\n",
      "        \"\"\"\n",
      "        self.w = np.random.randn(dim, 1) * 0.01\n",
      "        self.b = 0\n",
      "\n",
      "    def propagate(self, X, Y):\n",
      "        \"\"\"\n",
      "        forward propagation de la matrice X\n",
      "        X contient m exemples qui sont propag\u00e9s en une seule fois.\n",
      "        retourne le cost (moyenne des m erreurs)\n",
      "\n",
      "        utilise la fonction \"Cross-entropy loss\"\n",
      "        \"\"\"\n",
      "    \n",
      "        m = X.shape[1]\n",
      "        A = sigmoid(np.dot(self.w.T, X) + self.b)\n",
      "\n",
      "        cost = -1.0/m*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
      "\n",
      "        dw = 1/m*np.dot(X, (A - Y).T)\n",
      "        db = 1/m*np.sum((A - Y))\n",
      "\n",
      "        cost = np.squeeze(cost)\n",
      "\n",
      "        gradients = {\"dw\": dw,\n",
      "                 \"db\": db}\n",
      "\n",
      "        return gradients, cost\n",
      "    \n",
      "\n",
      "    def optimise(self, X, Y, nbiterations=2000, learning_rate=0.01, verbose=False):\n",
      "        \"\"\"\n",
      "        optimisation de w et b pour m exemples en m\u00eame temps.\n",
      "        \"\"\"\n",
      "\n",
      "        costs = []\n",
      "\n",
      "        for i in range(nbiterations):\n",
      "            #forward propagation\n",
      "            gradients, cost = self.propagate(X, Y)\n",
      "\n",
      "            #on r\u00e9cup\u00e8re les gradients\n",
      "            dw = gradients[\"dw\"]\n",
      "            db = gradients[\"db\"]\n",
      "            #on met \u00e0 jour w et b\n",
      "            self.w = self.w - learning_rate * dw\n",
      "            self.b = self.b - learning_rate * db\n",
      "\n",
      "            if i % 10 == 0:\n",
      "                costs.append(cost)\n",
      "\n",
      "        params = {\"w\": self.w,\n",
      "                  \"b\": self.b}\n",
      "\n",
      "        return params, costs\n",
      "\n",
      "    \n",
      "    \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        prediction sur un ensemble de m exemples\n",
      "        renvoie 0 ou 1\n",
      "        \"\"\"\n",
      "\n",
      "        m = X.shape[1]\n",
      "        Y_prediction = np.zeros((1,m))\n",
      "        w = self.w.reshape(X.shape[0], 1)\n",
      "\n",
      "        A = self.sigmoid(np.dot(w.T, X) + self.b)\n",
      "\n",
      "        for i in range(A.shape[1]):\n",
      "            Y_prediction = np.where(A>0.5, 1, 0)\n",
      "\n",
      "        return Y_prediction\n",
      "\n",
      "\n",
      "    def run(self, X_train, Y_train, X_test, Y_test, nbiterations=2000, learning_rate=0.01):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "    \n",
      "        self.initialiser(X_train.shape[0])\n",
      "    \n",
      "        parameters, costs = self.optimise(X_train, Y_train, nbiterations, learning_rate, verbose=False)\n",
      "    \n",
      "        self.w = parameters[\"w\"]\n",
      "        self.b = parameters[\"b\"]\n",
      "    \n",
      "        # calculer l'accuracy test / train\n",
      "        Y_prediction_test = self.predict(X_test)\n",
      "        Y_prediction_train = self.predict(X_train)\n",
      "    \n",
      "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
      "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
      "    \n",
      "        return parameters, costs, Y_prediction_test, Y_prediction_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class hiddenLayerPerceptron:\n",
      "    \"\"\"\n",
      "    impl\u00e9mente un r\u00e9seau de neurones \u00e0 un seul hidden layer.\n",
      "    mod\u00e8le : tanh -> sigmoid\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, inputdimension, hiddendim):\n",
      "        self.inputdimension = inputdimension\n",
      "        self.hiddendim = hiddendim\n",
      "        \n",
      "    def cost(self, A2, Y):\n",
      "        \"\"\"\n",
      "        retourne la moyenne de la cross-entropy loss\n",
      "        sur les m exemples contenus dans A\n",
      "        \"\"\"\n",
      "        m = Y.shape[1]\n",
      "        cost = - 1.0/m * np.sum(np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y)))\n",
      "\n",
      "        cost = np.squeeze(cost)    \n",
      "\n",
      "        return cost\n",
      "    \n",
      "    def initialize_parameters(self, X, Y):\n",
      "        \"\"\"\n",
      "\n",
      "        \"\"\"\n",
      "        # w et b du input layer :\n",
      "        w1 = np.random.randn(self.hiddendim, X.shape[0]) * 0.01\n",
      "        b1 = np.zeros((self.hiddendim, 1))\n",
      "\n",
      "        # w et b hiden layer :\n",
      "        w2 = np.random.randn(Y.shape[1], self.hiddendim) * 0.01\n",
      "        b2 = np.zeros((Y.shape[1], 1))\n",
      "\n",
      "        self.parameters = {\"w1\": w1,\"b1\": b1,\"w2\": w2,\"b2\": b2}\n",
      "\n",
      "    def forward_propagation(self, X):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        w1 = self.parameters[\"w1\"]\n",
      "        b1 = self.parameters[\"b1\"]\n",
      "        w2 = self.parameters[\"w2\"]\n",
      "        b2 = self.parameters[\"b2\"]\n",
      "\n",
      "        Z1 = np.dot(w1, X) + b1\n",
      "        A1 = np.tanh(Z1) #on utilise la fonction tanh de numpy\n",
      "        Z2 = np.dot(w2, A1) + b2\n",
      "        A2 = sigmoid(Z2) #en dernier, on utilise la fonction sigmoid\n",
      "        \n",
      "        return {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
      "    \n",
      "    def backward_propagation(self, cache, X, Y):\n",
      "        \"\"\"\n",
      "\n",
      "        \"\"\"\n",
      "        m = X.shape[1]\n",
      "\n",
      "        w1 = self.parameters[\"w1\"]\n",
      "        w2 = self.parameters[\"w2\"]\n",
      "\n",
      "        A1 = cache[\"A1\"]\n",
      "        A2 = cache[\"A2\"]\n",
      "\n",
      "        dZ2 = A2 - Y\n",
      "        dw2 = 1.0/m * np.dot(dZ2, A1.T)\n",
      "        db2 = 1.0/m * np.sum(dZ2, axis=1, keepdims=True)\n",
      "        dZ1 = np.dot(w2.T, dZ2) * (1 - np.power(A1, 2))\n",
      "        dw1 = 1.0/m * np.dot(dZ1, X.T)\n",
      "        db1 = 1.0/m * np.sum(dZ1, axis=1, keepdims=True)\n",
      "\n",
      "        return {\"dw1\": dw1,\"db1\": db1,\"dw2\": dw2,\"db2\": db2}\n",
      "    \n",
      "    def update_parameters(self, grads, learning_rate=0.01):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        w1 = self.parameters[\"w1\"]\n",
      "        b1 = self.parameters[\"b1\"]\n",
      "        w2 = self.parameters[\"w2\"]\n",
      "        b2 = self.parameters[\"b2\"]\n",
      "        \n",
      "        dw1 = grads[\"dw1\"]\n",
      "        db1 = grads[\"db1\"]\n",
      "        dw2 = grads[\"dw2\"]\n",
      "        db2 = grads[\"db2\"]\n",
      "\n",
      "        w1 = w1 - learning_rate * dw1\n",
      "        b1 = b1 - learning_rate * db1\n",
      "        w2 = w2 - learning_rate * dw2\n",
      "        b2 = b2 - learning_rate * db2\n",
      "\n",
      "        self.parameters = {\"w1\": w1,\"b1\": b1,\"w2\": w2,\"b2\": b2}\n",
      "        \n",
      "        \n",
      "    def train(self, X, Y, learning_rate=0.01, nbiterations=10000, verbose=False):\n",
      "        \"\"\"\n",
      "        entraine le mod\u00e8le\n",
      "        \"\"\"\n",
      "\n",
      "        self.initialize_parameters(X, Y)\n",
      "        costs = []\n",
      "\n",
      "        for i in range(nbiterations):\n",
      "\n",
      "            A2, cache = self.forward_propagation(X)\n",
      "\n",
      "            cost = self.cost(A2, Y)\n",
      "\n",
      "            grads = self.backward_propagation(cache, X, Y)\n",
      "\n",
      "            self.update_parameters(grads, learning_rate=learning_rate)\n",
      "            if i % 100 == 0:\n",
      "                costs.append(cost)\n",
      "            # Print the cost every 1000 iterations\n",
      "            if verbose and i % 1000 == 0:\n",
      "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
      "\n",
      "        return costs\n",
      "    \n",
      "    \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "\n",
      "        A2, cache = self.forward_propagation(X)\n",
      "        predicts = np.where(A2 >= 0.5, 1, 0)\n",
      "\n",
      "        return predict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "(20, 742543)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = hiddenLayerPerceptron(X_train.shape[0], 4)\n",
      "costs = nn.train(X_train, y_train, nbiterations=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "MemoryError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-16-647c990c7b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhiddenLayerPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbiterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-15-6972af091cd8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learning_rate, nbiterations, verbose)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbiterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-15-6972af091cd8>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#on utilise la fonction tanh de numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#en dernier, on utilise la fonction sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mMemoryError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 16
    }
   ],
   "metadata": {}
  }
 ]
}